Einleitung Eines der wichtigsten Ziele der Therapie von Sprechstörungen ist es, Verbesserungen zu erreichen, die sich auch im Alltag der Betroffenen zeigen. Vom Therapiezimmer aus lässt sich dies aber häufig nicht ausreichend beurteilen. Wichtige diagnostische Fragen können von therapeutischen Fachpersonen nicht valide beantwortet werden, wie zum Beispiel: Wird der Patient/die Patientin nach der Therapie besser verstanden? Wirkt das Sprechen auf andere natürlicher? Sind Veränderungen, die vielleicht akustisch messbar sind, für ein ungeschultes Ohr ebenfalls hörbar? Personen, die mit der Sprechweise einer Patientin/eines Patienten vertraut sind, eignen sich grundsätzlich nicht für eine objektive Beurteilung der Verständlichkeit, da ihr Urteil vom Grad ihrer Adaptation an die Sprechstörung der betroffenen Person abhängt. Dies gilt insbesondere für die behandelnden TherapeutInnen selbst, deren Verstehensleistung sich im Verlauf der Behandlung – unabhängig vom Therapieerfolg – zunehmend verbessert (Borrie, Lansford, & Barrett, 2017; DePaul & Kent, 2000; Lansford, Luhrsen, Ingvalson, & Borrie, 2018; Tjaden & Liss, 1995). ExpertInnen sind ausserdem auch für die Beurteilung so globaler und intuitiver Parameter wie der Natürlichkeit des Sprechens wenig geeignet, da sie trainiert sind, die funktionelle Symptomatik und die Störungsmechanismen zu identifizieren («analytisches Gehör») (Dagenais, Watts, Turnage, & Kennedy, 1999). Ihre Erfahrung und Expertise ist für die Analyse des Störungsmusters und der Störungsmechanismen und für die darauf aufbauende Entwicklung von Therapieansätzen unersetzlich. Für die Beurteilung der Verständlichkeitseinschränkung einer Patientin/eines Patienten und für die Einschätzung des Befremdens, das ihre/seine Sprechweise bei GesprächspartnerInnen auslösen kann, spielt diese Fachkenntnis aber eine untergeordnete Rolle. Hinzu kommt, dass auch die Einstellungen, die GesprächspartnerInnen gegenüber einer Person mit Sprechstörung haben zu Teilhabeeinschränkungen führen können (Schölderle, Staiger, Schumacher, & Ziegler, 2019). Um den Anspruch einer ökologisch validen Diagnostik solcher alltags- und kommunikationsrelevanter Störungsaspekte zu erfüllen, müssen also HörerInnen eingebunden werden, die mit den PatientInnen unvertraut sind und deren Urteil sich nicht analytisch an funktionellen Gesichtspunkten wie Sprechtempo, Stimmqualität oder Artikulationsschärfe orientiert, sondern den Eindruck ungeschulter Laien repräsentiert. Laienhörerurteile können darüber hinaus sinnvoll sein, wenn man für akustisch gemessene Verbesserungen überprüfen möchte, ob diese Veränderungen auch für das ungeschulte menschliche Ohr – also potenzielle GesprächspartnerInnen – hörbar sind («Perzeptuelle Validität») (Schölderle, Staiger, Hoffmann, & Ziegler, 2015). Man muss allerdings davon ausgehen, dass es für eine reliable Erfassung solcher Parameter viele Beurteilungen – also viele HörerInnen – braucht. Laien zu rekrutieren, und das noch in grosser Anzahl, ist mit den Abläufen in einer Klinik oder therapeutischen Praxis aber nicht zu vereinbaren. Eine Lösung für dieses Problem kommt aus dem Web: das Internet löst räumliche und zeitliche Distanzen auf und ermöglicht es, in kurzer Zeit viele Personen zu erreichen, die eine Bewertung abgeben können. Das Werkzeug dazu ist Crowdsourcing. 2. Crowdsourcing – was steckt dahinter? Der Begriff Crowdsourcing setzt sich etymologisch aus den Begriffen Crowd (Menschenmenge) und Outsourcing (Auslagerung) zusammen und meint die Auslagerung von Aufgaben an eine grosse Gruppe von unabhängigen Internetnutzern (Crowdworker) (Howe, 2006, 2008). Crowdsourcing hat viele Ausprägungen – eine davon ist das Microtasking. Grössere Projekte werden dabei meist in viele kleine, schnell zu erledigende Aufgaben (Microtasks) zerlegt und auf Online-Plattformen zur Bearbeitung angeboten. Für die Bearbeitung der Microtasks ist grösstenteils keine spezifische Expertise notwendig. Inzwischen gibt es weit mehr als 100 internationale Online-Plattformen, die Crowdworker mit unterschiedlicher Expertise vermitteln (Leimeister, Zogaj, Durward, & Blohm, 2016). Die wohl bekannteste internationale Microtasking-Plattform ist Amazon Mechanical Turk. Die deutschen Hauptanbieter sind Clickworker und Mylittlejob. 3. Studienlage zu Crowdsourcing im Bereich von Sprechstörungen Unter technischen Gesichtspunkten ist die Diagnostik von Sprechstörungen für webbasierte Methoden im Allgemeinen und für Crowdsourcing im Speziellen ein ideales Anwendungsgebiet, da Sprechproben ohne grossen Aufwand digital aufgenommen und daher unkompliziert für entsprechende Bewertungen zur Verfügung gestellt werden können. Die Studienlage zum Einsatz von Crowdsourcing zur Bewertung von (erworbenen und entwicklungsbedingten) Sprechstörungen ist allerdings noch sehr überschaubar: Unter Berücksichtigung der systematischen Literaturübersicht von Sescleifer, Francoisse und Lin (2018) zur auditiven Bewertung von Sprachäusserungen über Microtasking-Plattformen existieren für den klinischen Bereich (Einsatz von Crowdsourcing zur Bewertung von Sprechstörungen) insgesamt nur acht Studien, die Crowdsourcing zur Bewertung von Sprechstörungen einsetzten. Thematisch konzentrieren sich diese auf zwei Störungsbilder. Den ersten Bereich bilden Artikulationsstörungen bei Kindern und Jugendlichen (Harel, Hitchcock, Szeredi, Ortiz, & McAllister Byun, 2017; McAllister Byun, Halpin, & Szeredi, 2015; McAllister Byun, Harel, Halpin, & Szeredi, 2016; McAllister Byun, Hitchcock, & Harel, 2015). Die Studien hierzu stammen von einer Arbeitsgruppe und sind in ihrer Methodik gleich: Die Aufgabe der Crowdworker war es, die Realisierung des Ziellautes /r/ in Einzelwortäusserungen von Kindern und Jugendlichen (6 bis 16 Jahre) zu beurteilen, die sich in unterschiedlichen Phasen einer Artikulationstherapie bei Rhotazismus befanden. Die Bewertung musste entweder auf einer kontinuierlichen Skala (visuelle Analogskala mit den Ankerpunkten korrekt/inkorrekt) oder einer binären Skala (korrekt/inkorrekt) vorgenommen werden. Ein Aufgabenpaket für die HörerInnen umfasste dabei die Bewertung von 80 bis 120 Wörtern. Der zweite Bereich sind crowdbasierte Studien zu erworbenen Dysarthrien (Borrie et al., 2017; Lansford, Borrie, & Bystricky, 2016; McNaney et al., 2016; Nightingale, Swartz, Ramig, & McAllister Byun, 2016). Crowdsourcing wurde hier für unterschiedliche Fragestellungen eingesetzt: Lansford et al. (2016) und Borrie et al. (2017) untersuchten mit unterschiedlichen experimentellen Designs, ob ein auditives Training von HörerInnen zu einer perzeptuellen Adaptation an dysarthrisches Sprechen führt. Hintergrund beider Arbeiten ist die Frage, ob ein gezieltes Hörtraining zu einer Verbesserung der Verständlichkeit beitragen kann. Die Crowdworker fungierten in beiden Studien als Trainees. Nightingale et al. (2016) nutzten Crowdsourcing für eine diagnostische Fragestellung. Gegenstand war die Messung der Verbesserung der Artikulation von PatientInnen mit Dysarthrie bei Parkinson nach einer Intensivtherapie mit LSVT LOUD (Nightingale et al., 2016). Sie untersuchten dabei, ob die akustisch messbaren beziehungsweise die von ExpertInnen diagnostizierten Veränderungen auch von Laien wahrgenommen werden. Den CrowdhörerInnen wurden dazu randomisiert Wortpaare dargeboten, die jeweils vor und nach der Therapie aufgenommen worden waren. Es musste für jedes Paar die Entscheidung getroffen werden, welches Wort deutlicher artikuliert wurde. Eine Gruppe von 36 Crowdworkern bewertete auf diese Weise insgesamt 56 Wortpaare von 14 PatientInnen. Alle acht Veröffentlichungen stammen aus den letzten vier Jahren – Crowdsourcing ist also eine sehr neue Methode in diesem Bereich, die aber immer mehr an Bedeutung gewinnt, da sie sehr vielversprechend ist: Die meisten der genannten Studien verglichen die Crowdurteile mit den Daten aus akustischen Messungen und fanden durchgehend hohe Übereinstimmungen (Harel et al., 2017; McAllister Byun, Halpin, et al., 2015; McAllister Byun et al., 2016; McAllister Byun, Hitchcock, et al., 2015; Nightingale et al., 2016). Die AutorInnen konnten auf diese Weise einerseits die perzeptuelle Validität der angewandten akustischen Messungen nachweisen, andererseits aber auch die Praktikabilität von Crowdsourcing für die Messung von funktionell relevanten Störungsmerkmalen. Lansford et al. (2016) führten das bereits erwähnte Adapationsexperiment parallel auch unter «Laborbedingungen» durch. Die Trainees waren dann keine unbekannten InternetnutzerInnen, sondern eine vergleichbare Anzahl an Studierenden, die Training und Testung im Beisein der AutorInnen absolvierten. Dabei zeigte sich kein signifikanter Unterschied der Ergebnisse zwischen den Settings, jedoch ein hoher Kosten- und Zeitvorteil zugunsten der Erhebung über die Crowd (Crowd vs. Labor: Gesamtkosten $100 vs. $410; Zeitraum der Datenerhebung: 6 Stunden vs. 6 Monate). Abseits der eher experimentellen Studien ist nur ein Pilotprojekt bekannt, das ein Diagnostikverfahren für Dysarthrien erprobte, welches über die Verknüpfung mit Crowdworkern arbeitet (McNaney et al., 2016). Die Speeching-App ist eine Selfmonitoring-App für PatientInnen mit Parkinson mit der Idee, dass die Betroffenen ihre sprachlichen Äusserungen im Verlauf der Erkrankung im Hinblick auf die Parameter Verständlichkeit, subjektive Höranstrengung, Sprechstimmlage, Sprechlautstärke, und Sprechgeschwindigkeit von Laien bewerten lassen können. Der Nutzer/die Nutzerin spricht zehn Einzelwörter und drei Sätze ein und erhält dann visualisiert die Bewertungen von insgesamt fünf HörerInnen aus der Crowd. Die Supervision der Aufnahmen und die Interpretation der Ergebnisse durch therapeutisches Fachpersonal ist dabei nicht vorgesehen. Die SpeechingApp wurde bislang nur mit einer kleinen Stichprobe pilotiert. Dabei wurden von sechs PatientInnen mit Parkinson insgesamt 122 Sprachaufnahmesets (je 13 Äusserungen) gemacht und von einer Gruppe von 86 Crowdworkern bewertet. Durchschnittlich dauerte es pro Sitzung ca. 60 Minuten bis der Patient/die Patientin die Rückmeldung in Form der Crowdbewertungen ansehen konnte. Jeder Crowdworker bekam $0.42 für die Bewertung eines Testsets. Die Durchführung einer Untersuchung kostete bei fünf HörerInnen demnach $2.10. 4. Pilotstudie KommPaS-online KommPaS-online (KommunikationsParameter für Sprechstörungen) ist ein aktivitätsbezogenes Diagnostikverfahren für erwachsene PatientInnen mit Dysarthrie, das im Gegensatz zur dargestellten Speeching-App unter Supervision eines Therapeuten/einer Therapeutin durchgeführt wird. Es ist als technische Neuentwicklung und inhaltliche Erweiterung des Münchner Verständlichkeits-Profils (MVP) (Ziegler & Zierdt, 2008) entstanden. Mit KommPaS-online werden vier wichtige Indikatoren für die kommunikative Beeinträchtigung bei Dysarthrie webbasiert über Crowdworker beurteilt: (1) VERSTÄNDLICHKEIT, (2) NATÜRLICHKEIT, (3) SUBJEKTIVE HÖRANSTRENGUNG und (4) KOMMUNIKATIVE EFFIZIENZ. Konkret nimmt die/der Untersuchende über ein Online-Aufnahmetool (WebSpeechRecorder) (Draxler & Jänsch, 2004) eine circa 10-minütige Sprechprobe der Person mit Dysarthrie auf. Es handelt sich dabei um insgesamt 30 kurze Testsätze, die der Patient/die Patientin vorlesen oder nachsprechen muss. 27 Sätze dienen der Verständlichkeitsmessung. Sie werden durch Zugriff auf einen grossen Materialkorpus generiert und sind so konstruiert, dass die darin eingebetteten Zielwörter weder semantisch noch syntaktisch vorhersagbar sind. Die Zielwörter sind hinsichtlich ihrer Vorkommenshäufigkeit (niedrig-, mittel-, hochfrequent) und ihrer Länge (1–3 Silben) kontrolliert. Drei weitere, bedeutungstragende Sätze dienen der Beurteilung der NATÜRLICHKEIT der Sprachäusserungen. Die aufgezeichneten Sprechproben werden dann über eine Microtasking-Plattform des Anbieters Clickworker LaienhörerInnen präsentiert. Die HörerInnen werden zu Beginn der Hörersitzung detailliert instruiert. Zur Messung der Verständlichkeit haben die HörerInnen die Aufgabe, die 27 Zielwörter in Form einer Satzergänzungsaufgabe über die Computertastatur einzugeben. Ausserdem werden sie gebeten, auf einer visuellen Analogskala zu beurteilen, wie anstrengend sie es empfunden haben, die Patientin/den Patienten zu verstehen (SUBJEKTIVE HÖRANSTRENGUNG). Zur Messung des Parameters NATÜRLICHKEIT müssen die drei bedeutungstragenden Testsätze ebenfalls auf einer visuellen Analogskala beurteilt werden. Darüber hinaus wird die Dauer der gesprochenen Sätze semiautomatisch bestimmt und mit dem Anteil der korrekt verstandenen Wörter zur Berechnung eines Wertes zur KOMMUNIKATIVEN EFFIZIENZ verknüpft. Dieser Wert drückt den Zeitaufwand aus, den eine Sprecherin/ein Sprecher mit Dysarthrie benötigt, um sprachliche Information verständlich zu vermitteln. Der Auswertungsprozess wird zentral koordiniert und die Untersuchungsergebnisse von Fachpersonen validiert. Der Befund kann dann von der auftraggebenden klinischen Einrichtung online abgerufen werden. In einer Pilotstudie am Institut für Phonetik und Sprachverarbeitung (IPS) an der Ludwig-Maximilians-Universität (LMU) München wird KommPaS-online aktuell evaluiert. Im Projekt werden wichtige Fragen zur Testökonomie, zur Objektivität und Qualität der Crowdurteile sowie zur Reliabilität bearbeitet. Um Aussagen zur ökologischen Validität zu erhalten, werden patient reported outcomes (PROs) über einen Selbstbeurteilungsfragebogen erhoben und mit den KommPaS-Werten verglichen. 5. Kritische Betrachtung 5.1 Datenschutz- und Datensicherheit Nimmt man Sprachäusserungen von PatientInnen auf, handelt es sich – auch ohne inhaltlichen Bezug zur Person – um «besondere Kategorien personenbezogener Daten», da man aus dem akustischen Signal allein die Person erkennen könnte. Der Einsatz von Crowdsourcing als Methode für die Bewertung von Sprechstörungen bedeutet demnach, dass «sensible Daten» zum einen webbasiert verschickt werden und zum anderen, dass unbekannte HörerInnen diese Daten verarbeiten. Gemäss der Datenschutzgrundverordnung (DSGVO) bedarf es dazu einer transparenten Aufklärung der PatientInnen und einer eindeutigen Zustimmung im Rahmen einer Datenschutzerklärung. Im Sinne der Datensicherheit müssen ausserdem nach «Stand der Technik» Vorkehrungen getroffen werden, um die Daten vor dem Zugriff Unbefugter zu schützen. Dazu gehören beispielsweise die sichere Übertragung der Sprechproben im Internet über eine https-Verbindung, die pseudonymisierte Erhebung und Verarbeitung dieser Sprechproben, sowie ein Konzept für die sichere Datenspeicherung. Darüber hinaus muss für einen rechtskonformen Einsatz mit den Crowdworking-Firmen beziehungsweise mit den Crowdworkern selbst ein «Auftragsdatenverarbeitungsvertrag» (AVV) geschlossen werden. Dieser Vertrag regelt beispielsweise den Zweck und die Dauer der Verarbeitung. 5.2 Wer ist und was macht die Crowd tatsächlich? Ein Kritikpunkt an Crowdsourcing ist, dass man nur wenig Kontrolle über die Qualifikation der bewertenden Personen und den Bewertungsprozess an sich hat. Es lassen sich zwar Ausschlusskriterien (z.B. eine Hörstörung des Crowdworkers) und wichtige Hinweise zum Auswertungssetting (z.B. die Verwendung von Kopfhörern) formulieren – sicher nachprüfbar ist die Einhaltung dieser Vorgaben aber nicht. Die Tatsache, dass es sich bei den Bewertungen, die über eine weitgehend unbekannte Gruppe erhoben wurden, tendenziell um «verrauschte» Datensätze handelt, wird meist durch die Erhöhung der Stichprobengrösse kompensiert. Darüber hinaus gibt es Möglichkeiten zur Qualitätskontrolle, die sowohl vor, während als auch nach der Erledigung des Tasks eingesetzt werden können (Eskenazi, Levow, Meng, Parent, & Suendermann, 2013). AuftraggeberInnen haben die Möglichkeit ausgewählte Teams von Crowdworkern zu bilden, denen gezielt Aufträge angeboten werden können. Umgekehrt lassen sich auch Crowdworker ausschliessen, deren Arbeit nicht zufriedenstellend war. Die Qualität der Daten lässt sich aber auch durch Kontrollmassnahmen sicherstellen, indem man etwa bestimmte Kontrollaufgaben (catch trials) einschleust, deren fehlerhafte Bearbeitung zum Ausschluss des Crowdworkers führt. Die Aufgaben müssen ausserdem sehr präzise formuliert sein, damit keine Missverständnisse entstehen (Gerdenitsch & Korunka, 2019). Für die Frage wie sich die Crowd im Hinblick auf Aspekte wie Geschlechterverteilung, Ausbildungsgrad und berufliche Situation zusammensetzt, wird auf die systematische Befragung von mehreren hundert Crowdworkern in Deutschland durch Leimeister et al. (2016) verwiesen. Diese grossangelegte Studie widerlegt beispielsweise eindeutig die verbreitete Vermutung, dass es sich bei den Crowdworkern um berufliche unqualifizierte Personen handelt: ein Grossteil der Gruppe weist eine sehr gute schulische Ausbildung und fast die Hälfte einen akademischen Abschluss auf. Bezogen auf die Microtasking-Plattformen handelt es sich mehrheitlich um Selbstständige/FreiberuflerInnen und Studierende, die Crowdworking als flexible Arbeitsform neben ihrer Haupttätigkeit nutzen. 5.3 Crowdsourcing als kostengünstige Alternative zu herkömmlichen Diagnostikverfahren? Schneller, mehr und dabei noch günstiger – so wird die Datenerhebung über Crowdsourcing oft beschrieben. Die Kosteneffizienz ist aber gleichzeitig auch einer der Punkte, der am kritischsten zu betrachten ist. Die AuftraggeberInnen legen üblicherweise fest, wie hoch eine fertiggestellte Aufgabe entlohnt wird. Die Arbeit der einzelnen Personen in der Crowd muss in jedem Fall ausreichend – das heisst zumindest auf Mindestlohnniveau – bezahlt werden. Viele Plattformen geben Richtwerte für eine faire Bezahlung an. Entscheidend dafür ist, vorab zu eruieren, wie lange man für die Bearbeitung des Auftrages tatsächlich braucht, natürlich inklusive der Zeiten, die die Crowdworker beispielsweise für das Erfassen der Instruktion und die Bearbeitung von Übungsitems aufwenden müssen. Leimeister et al. (2016) liefern in ihrem Bericht auch eine umfangreiche Darstellung von Themen wie die Zufriedenheit mit den Arbeitsbedingungen und der Bezahlung aus Sicht der Crowdworker. Crowdsourcing ist also nicht die «billige» Alternative zu den herkömmlichen Diagnostikverfahren und auch in keinem Fall der kostengünstige Ersatz für die diagnostische Einschätzung des Therapeuten/der Therapeutin. Drei der bereits genannten Studien fassten eine Vielzahl an Crowdurteilen zusammen und verglichen das Ergebnis mit der Einschätzung von ExpertInnen (McAllister Byun, Halpin, et al., 2015; McNaney et al., 2016; Nightingale et al., 2016). Im Sinne der «Weisheit der Vielen» (Surowiecki, 2005) suggerieren die AutorInnen dadurch, dass die Bewertung durch ExpertInnen – bei zufriedenstellenden Korrelationsergebnissen – durch die Bewertung einer Crowd austauschbar ist. Dadurch werden zwei unterschiedliche Diagnostikbereiche vermischt, für die es auch unterschiedliche ExpertInnen gibt. Behandelnde TherapeutInnen sind ExpertInnen für die funktionelle Diagnostik und Entschlüsselung der Pathomechanismen wohingegen die LaienhörerInnen die ExpertInnen für aktivitätsbezogene Masse sind. Beide Einschätzungen sind komplementär und sollten zu einem umfassenden diagnostischen Bild zusammengefügt werden. 6. Schlussfolgerung Die ersten Vergleichsstudien zwischen Crowdsourcing und der herkömmlichen Datenerhebung unter Laborbedingungen im Bereich der Sprechpathologie zeigen, dass Crowdsourcing eine Methode ist, die auch das zu halten scheint, was sie verspricht: Es handelt sich nicht nur um viele Bewertungen in kurzer Zeit, sondern auch um qualitativ zufriedenstellende Bewertungen. Die Methode hat das Potenzial die funktionell-analytische Diagnostik durch das Ohr der ExpertInnen auf eine objektive Weise durch einen der wichtigsten Bausteine – die Einschätzung der kommunikativen Beeinträchtigung auf der Ebene der Aktivität – zu erweitern. Vorausgesetzt dabei ist die Berücksichtigung des Datenschutzes im Internet, eine faire Bezahlung der Laiendiagnostiker und eine Kontrolle der Untersuchungsergebnisse durch eine Fachperson.